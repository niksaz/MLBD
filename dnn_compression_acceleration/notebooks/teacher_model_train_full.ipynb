{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Teacher-Model\" data-toc-modified-id=\"Teacher-Model-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Teacher Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-preprocessing\" data-toc-modified-id=\"Data-preprocessing-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Data preprocessing</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-generator\" data-toc-modified-id=\"Data-generator-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Data generator</a></span></li><li><span><a href=\"#Set-hashing-space-for-each-sparse-field\" data-toc-modified-id=\"Set-hashing-space-for-each-sparse-field-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Set hashing space for each sparse field</a></span></li></ul></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Define-Model\" data-toc-modified-id=\"Define-Model-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Define Model</a></span></li><li><span><a href=\"#Train\" data-toc-modified-id=\"Train-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>Train</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Soft-targets\" data-toc-modified-id=\"Soft-targets-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Soft-targets</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teacher Model\n",
    "\n",
    "Обучим модель-учитель на всем датасете от Criteo.\n",
    "\n",
    "Модель Deep & Cross Network (DCN)\n",
    "\n",
    "Reference:\n",
    "```\n",
    "[1] Wang R, Fu B, Fu G, et al. Deep & cross network for ad click predictions[C]//Proceedings of the ADKDD'17. \n",
    "ACM, 2017: 12. (https://arxiv.org/abs/1708.05123)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from deepctr.inputs import SparseFeat, DenseFeat, get_feature_names\n",
    "from deepctr.models.dcn import DCN\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PREFIX = \"../../../data/criteo/\"\n",
    "\n",
    "FULL_TRAIN_DATA = os.path.join(DATA_PREFIX, 'train.txt')\n",
    "FULL_TEST_DATA = os.path.join(DATA_PREFIX, 'test.txt')\n",
    "\n",
    "TRAIN_DATA = os.path.join(DATA_PREFIX, 'train.csv')\n",
    "TEST_DATA = os.path.join(DATA_PREFIX, 'test.csv')\n",
    "TEST_LABELS_DATA = os.path.join(DATA_PREFIX, 'test_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_features_indices = [i for i in range(1, 14)]\n",
    "sparse_features_indices = [i for i in range(14, 40)]\n",
    "\n",
    "dense_features = ['c{}'.format(i) for i in dense_features_indices]\n",
    "sparse_features = ['c{}'.format(i) for i in sparse_features_indices]\n",
    "\n",
    "len(dense_features_indices), len(sparse_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_arr = [0] * 40\n",
    "max_arr = [0] * 40\n",
    "range_arr = [0] * 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator\n",
    "\n",
    "Для того чтобы не загружать весь файл целиком в память, будем читать его последовательно и генерировать батчи для обучения модели.\n",
    "\n",
    "Для dense фичей будем делать min-max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(filename, batch_size=1024, dense_feature_missing_value=0, sparse_feature_missing_value='-1'):\n",
    "    batch = defaultdict(list)\n",
    "    labels = defaultdict(list)\n",
    "    m = 0\n",
    "    \n",
    "    def prepare_data_dict(batch, dense_features, sparse_features):\n",
    "        data_dict = {}\n",
    "        for f_name in dense_features:\n",
    "            data_dict[f_name] = np.array(batch[f_name])\n",
    "\n",
    "        for f_name in sparse_features:\n",
    "            data_dict[f_name] = pd.core.series.Series(batch[f_name])\n",
    "\n",
    "        return data_dict\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            features = line.split('\\n')[0].split('\\t')\n",
    "            \n",
    "            labels['label'].append(np.int32(features[0]))\n",
    "            \n",
    "            for i, f_name in zip(dense_features_indices, dense_features):\n",
    "                val = features[i] if features[i] != '' else dense_feature_missing_value\n",
    "                val = float(val)\n",
    "                \n",
    "                min_arr[i] = min(min_arr[i], val)\n",
    "                max_arr[i] = max(max_arr[i], val)\n",
    "                range_arr[i] = max_arr[i] - min_arr[i]\n",
    "                \n",
    "                if range_arr[i]:\n",
    "                    val = (val - min_arr[i]) / range_arr[i]\n",
    "                \n",
    "                batch[f_name].append(val)\n",
    "                \n",
    "            for i, f_name in zip(sparse_features_indices, sparse_features):\n",
    "                val = features[i] if features[i] != '' else sparse_feature_missing_value\n",
    "                batch[f_name].append(val)\n",
    "            \n",
    "            m += 1\n",
    "            if m % batch_size == 0:\n",
    "                data_dict = prepare_data_dict(batch, dense_features, sparse_features)\n",
    "                yield data_dict, pd.core.series.Series(labels['label'])\n",
    "\n",
    "                m = 0\n",
    "                batch = defaultdict(list)\n",
    "                labels = defaultdict(list)\n",
    "\n",
    "        data_dict = prepare_data_dict(batch, dense_features, sparse_features)\n",
    "        yield data_dict, pd.core.series.Series(labels['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для валидации будем использовать те же данные, которые использовались при обучении маленькой модели учителя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "366493 366494\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(TRAIN_DATA)\n",
    "data.rename(columns=dict([(col, col[1:] if col[0] == '_' else col) for col in data.columns]), inplace=True)\n",
    "\n",
    "data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "data[dense_features] = data[dense_features].fillna(0, )\n",
    "target = ['c0']\n",
    "\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "_, test = train_test_split(data, test_size=0.2, shuffle=False)\n",
    "validation, test = train_test_split(test, test_size=0.5, shuffle=False)\n",
    "\n",
    "print(len(validation), len(test))\n",
    "\n",
    "\n",
    "def gen_model_input(df):\n",
    "    return {name: (pd.core.series.Series(df[name]) if name in sparse_features else np.array(df[name])) for name in feature_names}\n",
    "\n",
    "\n",
    "all_model_input = gen_model_input(data)\n",
    "validation_model_input = gen_model_input(validation)\n",
    "test_model_input = gen_model_input(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set hashing space for each sparse field\n",
    "\n",
    "Для категориальных фичей будем использовать Hashing Trick, чтобы уменьшить размерность словаря эмбеддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_features_dims = dict([\n",
    "    ('c14', 1445),\n",
    "    ('c15', 556),\n",
    "    ('c16', 1130758),\n",
    "    ('c17', 360209),\n",
    "    ('c18', 304),\n",
    "    ('c19', 21),\n",
    "    ('c20', 11845),\n",
    "    ('c21', 631),\n",
    "    ('c22', 3),\n",
    "    ('c23', 49223),\n",
    "    ('c24', 5194),\n",
    "    ('c25', 985420),\n",
    "    ('c26', 3157),\n",
    "    ('c27', 26),\n",
    "    ('c28', 11588),\n",
    "    ('c29', 715441),\n",
    "    ('c30', 10),\n",
    "    ('c31', 4681),\n",
    "    ('c32', 2029),\n",
    "    ('c33', 4),\n",
    "    ('c34', 870796),\n",
    "    ('c35', 17),\n",
    "    ('c36', 15),\n",
    "    ('c37', 87605),\n",
    "    ('c38', 84),\n",
    "    ('c39', 58187)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для sparse фичей указываем размерность словаря и размерность эмбеддинга.\n",
    "\n",
    "Размерность эмбеддинга вычисляется по формуле (см. [1])\n",
    "\n",
    "$$ \\text{embedding_dim} = 6 \\cdot (\\text{vocab_size})^{1/4} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixlen_feature_columns = [SparseFeat(feat, \n",
    "                                     vocabulary_size=min(vocab_size, 50000), \n",
    "                                     embedding_dim=min(int(6 * (vocab_size) ** (0.25)), 100), \n",
    "                                     use_hash=True, dtype='string') \n",
    "                          for feat, vocab_size in sparse_features_dims.items()] + \\\n",
    "                        [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "\n",
    "# fixlen_feature_columns = [SparseFeat(feat, \n",
    "#                                      vocabulary_size=min(vocab_size, 10000), \n",
    "#                                      embedding_dim=32, \n",
    "#                                      use_hash=True, dtype='string') \n",
    "#                           for feat, vocab_size in sparse_features_dims.items()] + \\\n",
    "#                         [DenseFeat(feat, 1,) for feat in dense_features]\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "\n",
    "Будем использовать гиперпараметры, описанные в [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCN(linear_feature_columns, dnn_feature_columns, cross_num=2,\n",
    "            dnn_hidden_units=(128, 128), l2_reg_linear=0, l2_reg_embedding=0,\n",
    "            l2_reg_cross=0, l2_reg_dnn=0, init_std=0.0001, seed=1024, \n",
    "            dnn_use_bn=True, dnn_activation='relu', task='binary')\n",
    "\n",
    "model.compile(\"adam\", \"binary_crossentropy\",\n",
    "              metrics=['binary_crossentropy'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"best_model_3.hdf5\", monitor='loss', verbose=1,\n",
    "                             save_best_only=True, mode='auto', period=1)\n",
    "\n",
    "model.fit(data_generator(FULL_TRAIN_DATA, 1024),\n",
    "          use_multiprocessing=True, steps_per_epoch=44766, epochs=5, verbose=1, \n",
    "          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(os.path.join(DATA_PREFIX, 'DCN_w.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model size**\n",
    "\n",
    "168MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Оценим качество модели на валидационной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'pandas.core.series.Series'>\", \"<class 'numpy.ndarray'>\"} values), <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "pred_ans = model.predict(test_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test LogLoss 0.4566\n",
      "test AUC 0.802\n"
     ]
    }
   ],
   "source": [
    "print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-targets\n",
    "\n",
    "Создадим новый таргет для обучения модели ученика. \n",
    "\n",
    "В качестве нового таргета будем использовать вероятность класса 1, которую нам выдает модель учитель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: (<class 'dict'> containing {\"<class 'str'>\"} keys and {\"<class 'pandas.core.series.Series'>\", \"<class 'numpy.ndarray'>\"} values), <class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(all_model_input, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = [p[0] for p in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'id': data['id'], 'prob': probs}) \\\n",
    "    .to_csv(os.path.join(DATA_PREFIX, 'soft_targets_full.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
